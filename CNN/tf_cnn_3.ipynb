{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c9a667-26fc-41e3-9cd8-d0b24241b12b",
   "metadata": {},
   "source": [
    "# CNN using Tensorflow Keras on MRI Image Data - failed attempt (memory out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c8148-eadf-4723-8f49-31d72a9f4867",
   "metadata": {},
   "source": [
    "## Data Use Agreements\n",
    "The data used for this project were provided in part by OASIS and ADNI.\n",
    "\n",
    "OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50 AG00561, P30 NS09857781, P01 AG026276, P01 AG003991, R01 AG043434, UL1 TR000448, R01 EB009352. AV-45 doses were provided by Avid Radiopharmaceuticals, a wholly owned subsidiary of Eli Lilly.\n",
    "\n",
    "Data collection for this project was done through the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e915b-3864-4342-bce3-26a43ed940a0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22b725-9eda-42d6-bb1d-a24ea7dd4b3b",
   "metadata": {},
   "source": [
    "### General Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa47e39-bdbf-455c-830d-4a846bd285f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel.freesurfer.mghformat as mgh\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os, sys, shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57203948-276a-43ee-8e82-477e6448f3e4",
   "metadata": {},
   "source": [
    "### Set up and test Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f6bfe6-91c3-4842-97bb-6a521ec333d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47ef8de-1285-452a-8447-34c723988e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hide the gpu so that it uses CPU, because GPU has OOM\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# if tf.test.gpu_device_name():\n",
    "#     print('GPU found')\n",
    "# else:\n",
    "#     print(\"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e319bc6-614a-446a-8c5d-989b128be282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.5.0\n",
      "Keras Version: 2.5.0\n",
      "\n",
      "Python 3.8.10 (default, Jun  4 2021, 15:09:15) \n",
      "[GCC 7.5.0]\n",
      "Pandas 1.2.4\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tf.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab211f4b-04d9-4683-a407-eb2af0dceb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x7fb6c52de5b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "sess.as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "496865be-e58b-426b-9683-1bf4d3ca906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# # Create some tensors\n",
    "# a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "# b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "# c = tf.matmul(a, b)\n",
    "\n",
    "# print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c79c7-fff2-4087-824f-5cf150986401",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4b384-8a21-4f4d-a985-94c9856caf36",
   "metadata": {},
   "source": [
    "### MRI\n",
    "Since all the files are already transformed via the freesurfer, I don't think we'll need to do any major preprocessing like cropping, flipping, or rotating.\n",
    "```\n",
    "main_directory/\n",
    "    control/\n",
    "        mr_id_001/\n",
    "            brain_image_001.mgz\n",
    "            brain_image_001_transformed.mgz\n",
    "            talairach_001.xfm\n",
    "        mr_id_002/\n",
    "            brain_image_002.mgz\n",
    "            brain_image_002_transformed.mgz\n",
    "            talairach_002.xfm\n",
    "    dementia/\n",
    "        mr_id_003/\n",
    "            brain_image_003.mgz\n",
    "            brain_image_003_transformed.mgz\n",
    "            talairach_003.xfm\n",
    "        mr_id_004/\n",
    "            brain_image_004.mgz\n",
    "            brain_image_004_transformed.mgz\n",
    "            talairach_004.xfm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5304427-5d5d-464a-98c5-78c88dbeffb4",
   "metadata": {},
   "source": [
    "#### Define some flags for general use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a291d44-19db-4f1f-9726-2da918e5facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class allows you to access dictionary items with a dot\n",
    "# Gathered from here: https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bfde2a-4ef5-4e02-9039-e9ada1b1df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scan_width': 256, 'scan_height': 256, 'scan_depth': 256, 'data_dir': '/home/jack/Code/GitHub/Polygence/Data/OASIS/mri_data', 'num_class': 4}\n"
     ]
    }
   ],
   "source": [
    "FLAGS = {\n",
    "    'scan_width'  : 256,\n",
    "    'scan_height' : 256,\n",
    "    'scan_depth'  : 256,\n",
    "    'data_dir'    : '/home/jack/Code/GitHub/Polygence/Data/OASIS/mri_data',\n",
    "    'num_class'   : 4\n",
    "}\n",
    "FLAGS = dotdict(FLAGS)\n",
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180ce6a-82d6-4e4d-ba00-fc67109b4e3f",
   "metadata": {},
   "source": [
    "#### Generate the filenames\n",
    "The returned filenames list will be organized as follows:\n",
    "```\n",
    "[[path_to_scan_1, label_for_scan_1],\n",
    " [path_to_scan_2, label_for_scan_2],\n",
    " [path_to_scan_3, label_for_scan_3],\n",
    " ...\n",
    " [path_to_scan_n, label_for_scan_n]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "440cdd48-2879-45da-8e22-d9d65a04a4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34aab3c6ec44e63918d4697fcccc75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "control:   0%|          | 0/712 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79916c96a66149f5b336c1b57ed157a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dementia:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_filenames(labels=['control', 'dementia'], random_state=1337):\n",
    "    pairs = []\n",
    "    \n",
    "    for label in labels:\n",
    "        label_dir = os.path.join(FLAGS.data_dir, label)\n",
    "        mr_ids = os.listdir(label_dir)\n",
    "        mr_ids.sort()\n",
    "        \n",
    "        for mr_id in tqdm(mr_ids, desc=label):\n",
    "            scans = os.path.join(label_dir, mr_id)\n",
    "            img_file = [file for file in os.listdir(scans) if \"transformed\" in file]\n",
    "            img_path = os.path.join(scans, img_file[0])\n",
    "            i = 1 if label == 'dementia' else 0\n",
    "            pairs.append([img_path, i])\n",
    "            \n",
    "    random.Random(random_state).shuffle(pairs)\n",
    "    \n",
    "    m = len(pairs)\n",
    "    filenames = []\n",
    "    labels = np.zeros((m, 1), dtype='int32')\n",
    "    \n",
    "    idx = 0\n",
    "    for filename, label in pairs:\n",
    "        filenames.append(filename)\n",
    "        labels[idx, 0] = label\n",
    "        idx += 1\n",
    "    \n",
    "    filenames = np.array(filenames)\n",
    "    return filenames, labels\n",
    "        \n",
    "\n",
    "X_filenames, y_labels = generate_filenames(random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "359df169-b7c7-4a6d-b691-96de1bb2b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these to load them later, in case we need it\n",
    "np.save('npy_files/X_filenames.npy', X_filenames)\n",
    "np.save('npy_files/y_labels.npy', y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee80f8f-8423-4ec3-aea9-1e1f49dbafb4",
   "metadata": {},
   "source": [
    "### Split into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c764a51c-fcc3-4c94-801a-72e58f23ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7192cea8-647d-40bc-9149-7218cb0e14bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filenames_shuffled, y_labels_shuffled = shuffle(X_filenames, y_labels, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64475ed5-e998-4942-82d1-015bc5ce30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filenames, X_test_filenames, y_train, y_test = train_test_split(\n",
    "    X_filenames_shuffled, y_labels_shuffled, test_size=.2, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd5b257-d1ee-47f1-9f51-106b093f7f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(817,) (817, 1)\n",
      ">> train >>  /home/jack/Code/GitHub/Polygence/Data/OASIS/mri_data/control/OAS30288_MR_d0897/OAS30288_Freesurfer50_d0897_brain_transformed.mgz [0]\n",
      "(205,) (205, 1)\n",
      ">> test >>  /home/jack/Code/GitHub/Polygence/Data/OASIS/mri_data/dementia/OAS30479_MR_d1266/OAS30479_Freesurfer53_d1266_brain_transformed.mgz [1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_filenames.shape, y_train.shape)\n",
    "print(\">> train >> \", X_train_filenames[0], y_train[0])\n",
    "print(X_test_filenames.shape, y_test.shape)\n",
    "print(\">> test >> \", X_test_filenames[5], y_test[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26aecc47-e80c-49f5-ad30-6b6608398026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save these splitted arrays for later as well\n",
    "np.save('npy_files/X_train_filenames.npy', X_train_filenames)\n",
    "np.save('npy_files/y_train.npy', y_train)\n",
    "\n",
    "np.save('npy_files/X_val_filenames.npy', X_test_filenames)\n",
    "np.save('npy_files/y_val.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12cc22-6284-462d-9f5d-f25234405df8",
   "metadata": {},
   "source": [
    "### Create a custom generator\n",
    "Since the data is too large to fit it all into memory, we will have to read it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59a71347-d514-4f20-9bd0-5ddb70459f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37bdaedb-5871-4e88-95fb-b66c6a2de84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize, rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c45d35-b95c-45ef-87ce-7b32a3eda30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRI_Data_Generator(keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    A data generator that reads MRI data in batches, and returns their image data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        \"\"\"\n",
    "        Intializes the generator\n",
    "        :param filenames: list containing the path to each MRI scan file, should be np.ndarray\n",
    "        :param labels: labels associated with the scans in filenames (control, dementia), should be np.ndarray\n",
    "        :param batch_size: the size of the batch\n",
    "        \"\"\"\n",
    "        self.filenames = filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculate the number of batches that we are supposed to produce.\n",
    "        Returns a rounded-up integer of total number of filenames divided by batch size.\n",
    "        \"\"\"\n",
    "        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Scan the data within that batch\n",
    "        :param idx: the index of the batch to be selected\n",
    "        \"\"\"\n",
    "        # Read in the items at that batch index\n",
    "        # Since these two arrays are np arrays, we don't have to worry about index_out_of_bounds\n",
    "        batch_X = self.filenames[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        batch_y = self.labels[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        \n",
    "        # Data preprocessing\n",
    "        def normalize(volume):\n",
    "            \"\"\" Normalize the volume, scaling it to [0, 1] instead of [0, 255] \"\"\"\n",
    "            min = 0.0\n",
    "            max = 255.0\n",
    "            volume[volume < min] = min\n",
    "            volume[volume > max] = max\n",
    "            volume = (volume - min) / (max - min)\n",
    "            volume = volume.astype(\"float32\")\n",
    "            return volume\n",
    "\n",
    "        def scale(volume):\n",
    "            \"\"\" Reduce the volume from (256,256,256) to (128,128,128) \"\"\"\n",
    "            return resize(volume, (128,128,128,1))\n",
    "            # return rescale(volume, 0.5)\n",
    "        \n",
    "        # print(f'Currently reading in batch {idx}')\n",
    "        \n",
    "        batch_X_data = []\n",
    "        for filename in batch_X:\n",
    "            rem_fs = filename[:filename.rfind(\"/\")]\n",
    "            rem_fs = rem_fs[rem_fs[:rem_fs.rfind(\"/\")].rfind(\"/\")+1:]\n",
    "#             print(f'Currently reading in batch {idx}: {rem_fs}')\n",
    "            \n",
    "            MRI_orig = mgh.load(filename)\n",
    "            volume = MRI_orig.get_fdata()\n",
    "            volume = normalize(volume)\n",
    "            volume = scale(volume)\n",
    "            batch_X_data.append(volume)\n",
    "        \n",
    "        np_res_X = np.array(batch_X_data)\n",
    "        np_res_y = np.array(batch_y)\n",
    "        # print(f'Shapes: x - {np_res_X.shape}, y - {np_res_y.shape}')\n",
    "        return np_res_X, np_res_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f2628-d250-4a00-9463-67536a68de82",
   "metadata": {},
   "source": [
    "### Create and test our data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73782594-bca4-4983-b05e-01e6ea6ba568",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# batch_size = 2\n",
    "\n",
    "training_batch_generator = MRI_Data_Generator(X_train_filenames, y_train, batch_size)\n",
    "testing_batch_generator = MRI_Data_Generator(X_test_filenames, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caa6d360-05ea-4cc4-b19e-6d83f4f308cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchx, batchy = training_batch_generator.__getitem__(0)\n",
    "# print(\">> Num of batches >> \", training_batch_generator.__len__())  # outputs 103, which is equal to ceil(817 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7782ea5-fb98-499d-bcc2-408337b39907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(batchx.shape, batchy.shape)\n",
    "# print(batchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2efa2a30-1b4f-4af3-bbfc-9e1a5a8d8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchx, batchy = testing_batch_generator.__getitem__(0)\n",
    "# print(\">> Num of batches >> \", testing_batch_generator.__len__()) # outputs 26, which is equal to ceil(205 / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0739f4e-e06c-44f6-a386-143672b785c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(batchx.shape, batchy.shape)\n",
    "# print(batchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "319e6174-4e82-4dd6-a328-5b2ade8048af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete them to save memory\n",
    "# del batchx, batchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffabf718-653d-4725-a1be-92cd56c5c615",
   "metadata": {},
   "source": [
    "## CNN Model\n",
    "We will have one convolutional layer, a maxpooling layer, then a final dense classification layer\n",
    "\n",
    "Resources used:\n",
    "* https://towardsdatascience.com/step-by-step-implementation-3d-convolutional-neural-network-in-keras-12efbdd7b130\n",
    "* https://medium.com/@mrgarg.rajat/training-on-large-datasets-that-dont-fit-in-memory-in-keras-60a974785d71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160a1e8-e00a-4bba-a62a-624b0c676931",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2949fb6b-3076-4d40-968c-2207acf4834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc758c65-a632-43c8-ba33-bb24e924dc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     model = keras.Sequential()\n",
    "\n",
    "#     # The input is 256x256x256x1: 256x256x256 for each scan, and 1 for the channel\n",
    "\n",
    "#     # The layer is a Conv3D layer that extracts 64 filters with a 5x5 window\n",
    "#     model.add(layers.Conv3D(filters=64, kernel_size=5, activation='relu', input_shape=(256,256,256,1)))\n",
    "\n",
    "#     # The second layer is a max-pooling layer with a 2x2 window for down sampling\n",
    "#     # I'm a little unsure of what max-pooling does exactly, but every resource I've seen used it after the Conv layer\n",
    "#     model.add(layers.MaxPool3D(pool_size=2))\n",
    "\n",
    "#     # Normalizes the batch\n",
    "#     model.add(layers.BatchNormalization())\n",
    "\n",
    "#     # Dropout layer to prevent overfitting\n",
    "#     # I'm also a little unsure with this, but again, every resource has used it\n",
    "#     model.add(layers.Dropout(0.25))\n",
    "\n",
    "#     # Layer to flatten the data\n",
    "#     # I'd like your input on this. Some resources have used it, while others have not.\n",
    "#     # This is commented out because it runs out of memory.\n",
    "#     # model.add(layers.Flatten())\n",
    "\n",
    "#     # Fully connected layers\n",
    "#     # In general, is going from 512 -> 1 too much of a sharp drop?\n",
    "#     model.add(layers.Dense(512, activation='relu'))\n",
    "#     # model.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "#     # Classification/output layer\n",
    "#     model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d018a74-5903-489c-a2ee-89347c8460ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = keras.Input((128, 128, 128, 1))\n",
    "\n",
    "    x = layers.Conv3D(filters=32, kernel_size=5, activation=\"relu\")(inputs)\n",
    "    x = layers.MaxPool3D(pool_size=2)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.25)(x)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling3D()(x)\n",
    "    x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs, name=\"3d-cnn\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d16002f-a8b9-4093-bf56-36354a1ffd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3d-cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 128, 1) 0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 124, 124, 124, 32) 4032      \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 62, 62, 62, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 62, 62, 62, 32)    128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 62, 62, 62, 32)    0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling3d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 12,865\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5893ba81-861c-4609-89b3-825b29eecc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa791f4-fc0c-48f8-9cde-d77c68cf347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04b24ed-e849-481b-84ad-392fdb885a2c",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43b14d95-ddcf-4d26-a3a9-57fa18703dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node 3d-cnn/conv3d/Conv3D (defined at <ipython-input-31-c7e657359a77>:1) ]] [Op:__inference_train_function_1226]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c7e657359a77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m                 \u001b[0mtraining_batch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_filenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# samples = batch_size * steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/polygence/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node 3d-cnn/conv3d/Conv3D (defined at <ipython-input-31-c7e657359a77>:1) ]] [Op:__inference_train_function_1226]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "                training_batch_generator,\n",
    "                steps_per_epoch = int(X_train_filenames.shape[0] // batch_size),  # samples = batch_size * steps\n",
    "                epochs = 10,\n",
    "                verbose = 1,\n",
    "                validation_data = testing_batch_generator,\n",
    "                validation_steps = int(X_test_filenames.shape[0] // batch_size))  # samples = batch_size * steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (polygence)",
   "language": "python",
   "name": "polygence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
