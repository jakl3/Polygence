{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1032e8-d506-4fa6-827d-5c9566297669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f1d0c9-0ddc-4bba-a5f1-6f442e45af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../Visualization/OASIS/oasis_3.csv')\n",
    "df = df.dropna(axis=1, how='all') # Drop any empty columns\n",
    "df = df.dropna(axis=0, how='any') # Drop any rows with empty values \n",
    "df = df.rename(columns={'id':'Freesurfer ID', 'dx1':'Diagnosis', \n",
    "                        'TOTAL_HIPPOCAMPUS_VOLUME':'TotalHippocampusVol'}) # Rename columns\n",
    "df = df.drop_duplicates(subset='Subject', keep='first') # Keep only the first visit; this is possible because\n",
    "                                                        # df is sorted by age\n",
    "df = df.reset_index(drop=True) # Reset the index\n",
    "df = df.set_index('Subject')\n",
    "cols = df.columns.tolist()\n",
    "cols[2], cols[4] = cols[4], cols[2]\n",
    "df = df[cols]\n",
    "df.loc[df['cdr'] < 0.5, 'Diagnosis'] = 'control'\n",
    "df.loc[~(df['cdr'] < 0.5), 'Diagnosis'] = 'dementia'\n",
    "df.loc[df['Diagnosis'] == 'control', 'Diagnosis'] = -1\n",
    "df.loc[df['Diagnosis'] == 'dementia', 'Diagnosis'] = 1\n",
    "df = df.drop(['MR ID', 'Freesurfer ID', 'M/F', 'cdr'], axis=1) # Drop categorical and redundant columns\n",
    "df = df.drop(['lhCortexVol', 'rhCortexVol', 'lhCorticalWhiteMatterVol', 'rhCorticalWhiteMatterVol', 'L.SurfArea', 'R.SurfArea'], axis=1) # Test drop to reduce dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d09392a-36f4-49d4-bd0d-b91d03ee73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Diagnosis'], axis=1)\n",
    "y = df['Diagnosis']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eebc8c40-2722-4fa6-a257-faf1ab5a3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard z score scaling\n",
    "def scale(X):\n",
    "    u = np.mean(X)\n",
    "    s = np.std(X)\n",
    "    X_scaled = (X-u)/s\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c6e4da-8875-4eda-bb0a-e9888fbdb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scale(X_train)\n",
    "X_test = scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ff42b9-24da-4ef3-961a-7b7407f6fe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age 0\n",
      "mmse 1\n",
      "apoe 2\n",
      "TotalHippocampusVol 3\n",
      "IntraCranialVol 4\n",
      "CortexVol 5\n",
      "SubCortGrayVol 6\n",
      "TotalGrayVol 7\n",
      "SupraTentorialVol 8\n",
      "CorticalWhiteMatterVol 9\n"
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(X_train):\n",
    "    print(col, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39579b83-d922-4bdd-b424-858cf5223138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #train with data\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # { |\\w\\|:{w,b}}\n",
    "        opt_dict = {}\n",
    "                    \n",
    "        self.max_feature_value = max(X.max(axis=0))         \n",
    "        self.min_feature_value = min(X.min(axis=0))  \n",
    "        \n",
    "        #with smaller steps our margins and db will be more precise\n",
    "        step_sizes = [self.max_feature_value * 0.1,\n",
    "                      self.max_feature_value * 0.01,\n",
    "                      #point of expense\n",
    "                      self.max_feature_value * 0.001,]\n",
    "        \n",
    "        #extremly expensise\n",
    "        b_range_multiple = 5\n",
    "        #we dont need to take as small step as w\n",
    "        b_multiple = 5\n",
    "        \n",
    "        latest_optimum = self.max_feature_value*10\n",
    "        \n",
    "        \"\"\"\n",
    "        objective is to satisfy yi(x.w)+b>=1 for all training dataset such that ||w|| is minimum\n",
    "        for this we will start with random w, and try to satisfy it with making b bigger and bigger\n",
    "        \"\"\"\n",
    "        #making step smaller and smaller to get precise value\n",
    "        for step in step_sizes:\n",
    "            w = np.full(shape=X.shape[1], fill_value=latest_optimum)\n",
    "            \n",
    "            #we can do this because convex\n",
    "            optimized = False\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1*self.max_feature_value*b_range_multiple,\n",
    "                                   self.max_feature_value*b_range_multiple,\n",
    "                                   step*b_multiple):\n",
    "                    #w_t = w*transformation\n",
    "                    found_option = True\n",
    "                    w_t = w\n",
    "\n",
    "                    #weakest link in SVM fundamentally\n",
    "                    #SMO attempts to fix this a bit\n",
    "                    # ti(xi.w+b) >=1\n",
    "                    for index, row in X.iterrows():\n",
    "                        yi = y[index]\n",
    "                        if not abs(yi*(np.dot(w_t,row)+b))>=1:\n",
    "                            #print(yi*(np.dot(w_t,row)+b))\n",
    "                            found_option=False\n",
    "                    if found_option:\n",
    "                        \"\"\"\n",
    "                        all points in dataset satisfy y(w.x)+b>=1 for this current w_t, b\n",
    "                        then put w,b in dict with ||w|| as key\n",
    "                        \"\"\"\n",
    "                        opt_dict[np.linalg.norm(w_t)]=[w_t,b]\n",
    "                        #print([w_t,b])\n",
    "                \n",
    "                #after w[0] or w[1]<0 then values of w starts repeating itself because of transformation\n",
    "                #Think about it, it is easy\n",
    "                #print(w,len(opt_dict)) Try printing to understand\n",
    "                if w[0]<0:\n",
    "                    optimized=True\n",
    "                    #print(\"optimized a step\")\n",
    "                else:\n",
    "                    w = w-step\n",
    "                    \n",
    "            # sorting ||w|| to put the smallest ||w|| at poition 0 \n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            #optimal values of w,b\n",
    "            opt_choice = opt_dict[norms[0]]\n",
    "\n",
    "            self.w=opt_choice[0]\n",
    "            self.b=opt_choice[1]\n",
    "            \n",
    "            #start with new latest_optimum (initial values for w)\n",
    "            latest_optimum = opt_choice[0][0]+step*2\n",
    "    \n",
    "    def predict(self,features):\n",
    "        #sign(x.w+b)\n",
    "        classification = np.sign(np.dot(np.array(features),self.w)+self.b)\n",
    "        return (classification,np.dot(np.array(features),self.w)+self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9765e7b2-5a07-42b0-a4e1-93ff973ba52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVM()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e17546-43dc-4a42-99bb-91d7ba2c7c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "(511,)\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)[0]\n",
    "print(np.sum(predictions != np.array(y_test)))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af2f3c-d11e-4173-b6c2-f3819363f906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
